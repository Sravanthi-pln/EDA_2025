{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sravanthi-pln/EDA_2025/blob/main/5_LangChain_Prompt_Tempate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLSJYuvbb_hl"
      },
      "source": [
        "# What is PromptTemplate?\n",
        "\n",
        "- PromptTemplate in LangChain helps you create reusable and dynamic prompts for LLMs.\n",
        "\n",
        "Instead of hardcoding a string like:\n",
        "\n",
        "\"Summarize this: LangChain is a tool for building with LLMs\"\n",
        "\n",
        "\n",
        "\"Summarize this: {text}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-Ouy4Web2Uf"
      },
      "outputs": [],
      "source": [
        "# !pip install chromadb # is used to save vectors Database\n",
        "# !pip install langchain\n",
        "# !pip install langchain-community\n",
        "# !pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUOKydAM9Hsu"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHgDSC9t9Hsv"
      },
      "source": [
        "**With Out Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY-MHILS9Hsv",
        "outputId": "4b12f9aa-e571-4651-a9e7-6ad770ee79f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a fun fact about penguins:\n",
            "\n",
            "Penguins can \"porpoise,\" which means they leap out of the water and back in repeatedly while swimming. This helps them gain speed, breathe, and avoid predators!\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=\"AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8\",\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "response=llm.invoke(\"Give me a fun fact about penguins.\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Khtorg39Hsw"
      },
      "source": [
        "**With Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjn9hM9N9Hsw"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a fun fact about {topic}.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hsIWBBl9Hsw"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',\n",
        "    temperature=0.3,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qjA56xJ9Hsw",
        "outputId": "85a2369c-e50e-417c-df26-2d055d2065f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a fun fact about penguins:\n",
            "\n",
            "Penguins can \"porpoise,\" which means they leap out of the water repeatedly while swimming. This helps them breathe, avoid predators, and travel faster!\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "# Step 4: Run the chain\n",
        "response = chain.invoke(\"penguins\") #chain.run(\"penguins\")\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAk_lzfrb6SW",
        "outputId": "c6c473e8-30d1-4ab1-eae5-54ef0fe7912b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-c418d18ffb9e>:19: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-1-c418d18ffb9e>:22: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run(\"penguins\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a fun fact about penguins:\n",
            "\n",
            "Penguins can \"porpoise\" â€“ meaning they leap out of the water and back in repeatedly while swimming. This helps them gain speed, breathe, and avoid predators!\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Step 1: Create the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a fun fact about {topic}.\"\n",
        ")\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "# Step 3: Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Step 4: Run the chain\n",
        "response = chain.run(\"penguins\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHKMc_EOb-2O"
      },
      "source": [
        "# For QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m65Iex8KdIXq",
        "outputId": "a9756553-9f93-45c8-f8e0-b0ac4c020674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mughal Emperor Shah Jahan built the Taj Mahal.\n"
          ]
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Use the context below to answer the question:\\n\\nContext: {context}\\n\\nQuestion: {question}\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "context = \"The Taj Mahal is located in Agra and was built by Mughal Emperor Shah Jahan.\"\n",
        "question = \"Who built the Taj Mahal?\"\n",
        "\n",
        "print(chain.run({\"context\": context, \"question\": question}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1awVDVc-dMXY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}